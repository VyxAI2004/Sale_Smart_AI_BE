"""
Shopee Scraper - Hybrid Approach
S·ª≠ d·ª•ng Selenium + Saved Cookies ƒë·ªÉ crawl

Flow:
1. Ch·∫°y scripts/shopee_login.py ƒë·ªÉ login v√† save cookies (1 l·∫ßn)
2. Scraper load cookies v√†o Selenium (skip login)
3. Parse HTML t·ª´ rendered page
"""

import re
import time
import urllib.parse
import json
import logging
import requests
from typing import List, Optional
from pathlib import Path

import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from services.features.product_intelligence.crawler.base_scraper import BaseScraper
from services.features.product_intelligence.crawler.cookie_manager import CookieManager
from schemas.product_crawler import CrawledProductItem, CrawledProductDetail, CrawledReview

# Setup logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


class ShopeeScraper(BaseScraper):
    """
    Shopee Scraper s·ª≠ d·ª•ng Selenium + Saved Cookies
    Y√™u c·∫ßu: Ch·∫°y scripts/shopee_login.py tr∆∞·ªõc ƒë·ªÉ l·∫•y cookies
    """
    
    def __init__(self, account_name: str = "default"):
        self.base_url = "https://shopee.vn"
        self.cookie_manager = CookieManager(account_name)
        self.account_name = account_name
        logger.info(f"ShopeeScraper initialized with account: {account_name}")
    
    def _auto_login_internal(self, timeout_seconds: int = 120) -> bool:
        """
        Auto login n·ªôi b·ªô - m·ªü browser, ch·ªù user login, l∆∞u cookies
        
        Returns:
            True n·∫øu login th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        driver = None
        
        try:
            print(f"\n{'='*60}")
            print(f"[SHOPEE AUTO LOGIN] üîê Starting auto login")
            print(f"[SHOPEE AUTO LOGIN] ‚è∞ Timeout: {timeout_seconds}s")
            print(f"{'='*60}")
            
            options = uc.ChromeOptions()
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--lang=vi-VN")
            options.add_argument("--start-maximized")
            
            driver = uc.Chrome(options=options, version_main=None)
            print("[SHOPEE AUTO LOGIN] ‚úÖ Browser opened")
            
            # Navigate to login page
            print("[SHOPEE AUTO LOGIN] üåê Loading login page...")
            driver.get("https://shopee.vn/buyer/login")
            time.sleep(3)
            
            print("[SHOPEE AUTO LOGIN] ‚è≥ Waiting for you to login...")
            print("[SHOPEE AUTO LOGIN] üí° Login b·∫±ng OTP, SMS ho·∫∑c Password")
            print("-" * 60)
            
            # Poll for login completion
            start_time = time.time()
            check_interval = 2
            
            while True:
                elapsed = time.time() - start_time
                
                if elapsed > timeout_seconds:
                    print(f"\n[SHOPEE AUTO LOGIN] ‚è∞ Timeout sau {timeout_seconds}s")
                    return False
                
                try:
                    current_url = driver.current_url
                    
                    # Log progress m·ªói 10 gi√¢y
                    if int(elapsed) % 10 == 0 and int(elapsed) > 0:
                        remaining = timeout_seconds - int(elapsed)
                        print(f"[SHOPEE AUTO LOGIN] ‚è≥ ƒêang ch·ªù... ({remaining}s c√≤n l·∫°i)")
                    
                    # Check if login successful
                    if "/login" not in current_url and "/buyer/login" not in current_url:
                        print(f"\n[SHOPEE AUTO LOGIN] ‚úÖ Login detected!")
                        
                        time.sleep(3)
                        
                        # Check for traffic verification
                        current_url = driver.current_url
                        if "/verify/traffic" in current_url:
                            print("[SHOPEE AUTO LOGIN] ‚ö†Ô∏è  Traffic verification detected")
                            time.sleep(10)
                            if "/verify/traffic" in driver.current_url:
                                return False
                        
                        # Get and save cookies
                        cookies = driver.get_cookies()
                        if not cookies:
                            return False
                        
                        self.cookie_manager.save_cookies(cookies)
                        print(f"[SHOPEE AUTO LOGIN] üíæ Saved {len(cookies)} cookies")
                        print("[SHOPEE AUTO LOGIN] ‚úÖ SUCCESS!")
                        print("=" * 60)
                        
                        return True
                    
                except Exception as e:
                    logger.debug(f"Check error: {e}")
                
                time.sleep(check_interval)
            
        except Exception as e:
            print(f"[SHOPEE AUTO LOGIN] ‚ùå Error: {str(e)}")
            return False
        finally:
            if driver:
                try:
                    driver.quit()
                    print("[SHOPEE AUTO LOGIN] üîí Browser closed")
                except:
                    pass

    def crawl_search_results(self, search_url: str, max_products: int = 10) -> List[CrawledProductItem]:
        """
        Crawl search results s·ª≠ d·ª•ng Selenium + Saved Cookies
        """
        print(f"\n{'='*60}")
        print(f"[SHOPEE] üîç Crawling search results")
        print(f"[SHOPEE] URL: {search_url}")
        print(f"{'='*60}")
        
        # Extract query t·ª´ URL
        query = search_url
        if "shopee.vn/search" in search_url:
            try:
                parsed = urllib.parse.urlparse(search_url)
                qs = urllib.parse.parse_qs(parsed.query)
                query = qs.get("keyword", [search_url])[0]
            except Exception:
                pass
        
        if not query or not query.strip():
            print("[SHOPEE] ‚ùå Empty query")
            return []
        
        print(f"[SHOPEE] üîé Query: {query}")
        
        # Load cookies
        saved_cookies = self.cookie_manager.load_cookies()
        has_cookies = saved_cookies and len(saved_cookies) > 0
        
        if has_cookies:
            print(f"[SHOPEE] üç™ Found {len(saved_cookies)} saved cookies")
        else:
            print("[SHOPEE] ‚ö†Ô∏è  No saved cookies found")
            print("[SHOPEE] ÔøΩ Auto-login s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán...")
            
            # Auto login
            login_result = self._auto_login_internal()
            
            if login_result:
                saved_cookies = self.cookie_manager.load_cookies()
                has_cookies = saved_cookies and len(saved_cookies) > 0
                print(f"[SHOPEE] ‚úÖ Auto-login th√†nh c√¥ng! {len(saved_cookies)} cookies")
            else:
                print("[SHOPEE] ‚ùå Auto-login th·∫•t b·∫°i!")
                return []
        
        # Build search URL
        encoded_query = urllib.parse.quote(query)
        search_page_url = f"{self.base_url}/search?keyword={encoded_query}"
        
        driver = None
        try:
            # Initialize undetected-chromedriver
            print("[SHOPEE] üöÄ Starting browser...")
            
            options = uc.ChromeOptions()
            # KH√îNG d√πng headless - Shopee detect v√† block
            # options.add_argument("--headless=new")  # DISABLED
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--lang=vi-VN")
            options.add_argument("--start-maximized")
            
            driver = uc.Chrome(options=options, version_main=None)
            print("[SHOPEE] ‚úÖ Browser started (NON-HEADLESS)")
            
            # Load homepage first ƒë·ªÉ set cookies
            print("[SHOPEE] üè† Loading homepage...")
            driver.get(self.base_url)
            time.sleep(3)
            
            # Apply saved cookies
            if has_cookies:
                print("[SHOPEE] üç™ Applying saved cookies...")
                for cookie in saved_cookies:
                    try:
                        cookie_dict = {
                            'name': cookie.get('name'),
                            'value': cookie.get('value'),
                            'domain': cookie.get('domain', '.shopee.vn'),
                            'path': cookie.get('path', '/'),
                        }
                        # Only add if name and value exist
                        if cookie_dict['name'] and cookie_dict['value']:
                            driver.add_cookie(cookie_dict)
                    except Exception as e:
                        logger.debug(f"Failed to add cookie: {e}")
                
                print(f"[SHOPEE] ‚úÖ Applied cookies")
                
                # Refresh ƒë·ªÉ apply cookies
                driver.refresh()
                time.sleep(3)
            
            # Now load search page
            print(f"[SHOPEE] üîç Loading search page...")
            driver.get(search_page_url)
            time.sleep(5)
            
            # Check current URL
            current_url = driver.current_url
            page_title = driver.title
            print(f"[SHOPEE] üìÑ Page loaded")
            print(f"[SHOPEE]    URL: {current_url[:80]}...")
            print(f"[SHOPEE]    Title: {page_title}")
            
            # Check for login/verification redirect
            if "/login" in current_url:
                print("[SHOPEE] ‚ö†Ô∏è  Login required - cookies expired!")
                print("[SHOPEE] üí° Run: POST /api/v1/shopee/session/auto-login")
                self.cookie_manager.clear_cookies()
                return []
            
            # Handle captcha verification
            if "/verify/captcha" in current_url:
                print("[SHOPEE] ‚ö†Ô∏è  Captcha verification detected!")
                print("[SHOPEE] üëÜ Vui l√≤ng gi·∫£i captcha trong browser...")
                print("[SHOPEE] ‚è≥ ƒêang ch·ªù b·∫°n ho√†n th√†nh captcha (t·ªëi ƒëa 120s)...")
                
                # Wait for user to solve captcha - TƒÇNG L√äN 120s
                captcha_timeout = 120
                start_time = time.time()
                captcha_solved = False
                
                while time.time() - start_time < captcha_timeout:
                    time.sleep(2)
                    
                    try:
                        current_url = driver.current_url
                    except Exception as e:
                        print(f"[SHOPEE] ‚ö†Ô∏è  Browser error: {e}")
                        print("[SHOPEE] üí° Browser c√≥ th·ªÉ ƒë√£ b·ªã ƒë√≥ng, th·ª≠ l·∫°i...")
                        return []
                    
                    remaining = int(captcha_timeout - (time.time() - start_time))
                    if remaining % 20 == 0 and remaining > 0:
                        print(f"[SHOPEE] ‚è≥ C√≤n {remaining}s ƒë·ªÉ gi·∫£i captcha...")
                    
                    if "/verify/captcha" not in current_url:
                        print("[SHOPEE] ‚úÖ Captcha solved!")
                        captcha_solved = True
                        # Update cookies after captcha
                        try:
                            new_cookies = driver.get_cookies()
                            self.cookie_manager.save_cookies(new_cookies)
                            print(f"[SHOPEE] üíæ Updated cookies after captcha")
                        except:
                            pass
                        break
                
                if not captcha_solved:
                    print("[SHOPEE] ‚è∞ Captcha timeout sau 120s!")
                    print("[SHOPEE] üí° Th·ª≠ l·∫°i - b·∫°n c√≥ 2 ph√∫t ƒë·ªÉ gi·∫£i captcha")
                    return []
                
                # Check if now on search page
                try:
                    current_url = driver.current_url
                    if "/search" not in current_url:
                        print(f"[SHOPEE] üîÑ Reloading search page...")
                        driver.get(search_page_url)
                        time.sleep(5)
                except Exception as e:
                    print(f"[SHOPEE] ‚ö†Ô∏è  Error after captcha: {e}")
                    return []
            
            # Handle traffic verification  
            if "/verify/traffic" in current_url:
                print("[SHOPEE] ‚ö†Ô∏è  Traffic verification detected!")
                print("[SHOPEE] üîÑ Waiting for manual verification...")
                print("[SHOPEE] üí° B·∫°n c√≥ th·ªÉ c·∫ßn verify trong browser popup")
                
                # Ch·ªù user verify (n·∫øu c√≥ captcha)
                time.sleep(10)
                
                # Th·ª≠ l·∫°i
                driver.get(search_page_url)
                time.sleep(5)
                
                current_url = driver.current_url
                if "/verify/traffic" in current_url:
                    print("[SHOPEE] ‚ùå Still blocked!")
                    print("[SHOPEE] üí° Solutions:")
                    print("[SHOPEE]    1. Wait 5-10 minutes and try again")
                    print("[SHOPEE]    2. Use a VPN to change IP")
                    print("[SHOPEE]    3. Login again: POST /api/v1/shopee/session/auto-login")
                    return []
                else:
                    print("[SHOPEE] ‚úÖ Traffic verification passed!")
            
            # Check for error page
            page_source = driver.page_source
            if "s·ª± c·ªë t·∫£i" in page_source or "th·ª≠ l·∫°i" in page_source or "error" in current_url.lower():
                print("[SHOPEE] ‚ö†Ô∏è  Shopee ƒëang g·∫∑p s·ª± c·ªë ho·∫∑c rate limiting!")
                print("[SHOPEE] üîÑ Retrying in 5 seconds...")
                
                time.sleep(5)
                driver.refresh()
                time.sleep(5)
                
                # Check again
                page_source = driver.page_source
                if "s·ª± c·ªë t·∫£i" in page_source or "th·ª≠ l·∫°i" in page_source:
                    print("[SHOPEE] ‚ùå V·∫´n b·ªã l·ªói sau khi retry!")
                    print("[SHOPEE] üí° Gi·∫£i ph√°p:")
                    print("[SHOPEE]    1. ƒê·ª£i 5-10 ph√∫t r·ªìi th·ª≠ l·∫°i")
                    print("[SHOPEE]    2. D√πng VPN ƒë·ªÉ ƒë·ªïi IP")
                    print("[SHOPEE]    3. IP c·ªßa b·∫°n c√≥ th·ªÉ ƒëang b·ªã rate limit")
                    return []
                else:
                    print("[SHOPEE] ‚úÖ Retry th√†nh c√¥ng!")
            
            # Scroll to load products
            print("[SHOPEE] üìú Scrolling to load products...")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(2)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # Update cookies
            new_cookies = driver.get_cookies()
            if len(new_cookies) > 0:
                self.cookie_manager.save_cookies(new_cookies)
                print(f"[SHOPEE] üíæ Updated {len(new_cookies)} cookies")
            
            # Parse products from HTML
            print("[SHOPEE] üîé Parsing products from HTML...")
            products = self._parse_search_results(driver.page_source, max_products)
            
            print(f"[SHOPEE] ‚úÖ Found {len(products)} products")
            for i, p in enumerate(products[:3], 1):
                print(f"[SHOPEE]    {i}. {p.name[:50]}...")
            
            return products
            
        except Exception as e:
            print(f"[SHOPEE] ‚ùå Error: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass

    def _parse_search_results(self, html: str, max_products: int = 10) -> List[CrawledProductItem]:
        """Parse products t·ª´ HTML search results"""
        try:
            from bs4 import BeautifulSoup
        except ImportError:
            print("[SHOPEE] ‚ùå BeautifulSoup not installed")
            return []
        
        soup = BeautifulSoup(html, "html.parser")
        results = []
        
        # Shopee product selectors (may change)
        selectors = [
            # Selector 1: Product links with -i. pattern
            ("a", {"href": lambda x: x and "-i." in x}),
            # Selector 2: data-sqe attribute
            ("div", {"data-sqe": "link"}),
        ]
        
        product_links = []
        for tag, attrs in selectors:
            found = soup.find_all(tag, attrs)
            if found:
                product_links = found
                print(f"[SHOPEE] Found {len(found)} items with selector: {tag}, {attrs}")
                break
        
        if not product_links:
            print("[SHOPEE] ‚ö†Ô∏è  No products found in HTML")
            # Debug: print first 1000 chars
            print(f"[SHOPEE] HTML preview: {html[:1000]}")
            return []
        
        seen_links = set()
        
        for elem in product_links:
            if len(results) >= max_products:
                break
            
            try:
                # Get link
                if elem.name == "a":
                    link = elem.get("href", "")
                else:
                    link_elem = elem.find("a", href=lambda x: x and "-i." in x)
                    link = link_elem.get("href", "") if link_elem else ""
                
                if not link or "-i." not in link:
                    continue
                
                # Normalize link
                if link.startswith("/"):
                    link = self.base_url + link
                elif not link.startswith("http"):
                    link = self.base_url + "/" + link
                
                # Skip duplicates
                if link in seen_links:
                    continue
                seen_links.add(link)
                
                # Get parent container for more info
                parent = elem.find_parent("div") or elem
                
                # Get name
                name = ""
                name_elem = parent.find("div", class_=lambda x: x and "line-clamp" in str(x))
                if name_elem:
                    name = name_elem.get_text(strip=True)
                if not name:
                    name = elem.get("title") or elem.get_text(strip=True)
                
                if not name or len(name) < 5:
                    continue
                
                # Get price
                price = None
                price_elem = parent.find("span", string=re.compile(r'‚Ç´|ƒë|\d'))
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    # Extract numbers
                    numbers = re.findall(r'[\d.,]+', price_text)
                    if numbers:
                        try:
                            price = float(numbers[0].replace('.', '').replace(',', ''))
                        except:
                            pass
                
                # Get image
                img = None
                img_elem = parent.find("img")
                if img_elem:
                    img = img_elem.get("src") or img_elem.get("data-src")
                    if img and not img.startswith("http"):
                        img = "https:" + img if img.startswith("//") else None
                
                results.append(CrawledProductItem(
                    name=name,
                    price=price,
                    sold=None,
                    rating=None,
                    img=img,
                    link=link,
                    platform="shopee"
                ))
                
            except Exception as e:
                logger.debug(f"Error parsing product: {e}")
                continue
        
        return results

    def _extract_ids(self, url: str) -> tuple[Optional[str], Optional[str]]:
        """Extract shopid v√† itemid t·ª´ URL"""
        m = re.search(r"i\.(\d+)\.(\d+)", url)
        if m:
            return m.group(1), m.group(2)
        return None, None

    def crawl_product_details(self, product_url: str, review_limit: int = 30) -> CrawledProductDetail:
        """
        Crawl product details v√† reviews
        ∆Øu ti√™n API (nhanh) ‚Üí Fallback Selenium n·∫øu c·∫ßn
        """
        print(f"\n{'='*60}")
        print(f"[SHOPEE] üì¶ Crawling product details")
        print(f"[SHOPEE] URL: {product_url}")
        print(f"{'='*60}")
        
        shopid, itemid = self._extract_ids(product_url)
        if not shopid or not itemid:
            print(f"[SHOPEE] ‚ùå Cannot extract IDs from URL")
            return CrawledProductDetail(link=product_url)
        
        print(f"[SHOPEE] üîë Shop ID: {shopid}, Item ID: {itemid}")
        
        # Load cookies
        saved_cookies = self.cookie_manager.load_cookies()
        
        if not saved_cookies:
            print("[SHOPEE] ‚ùå No cookies found!")
            print("[SHOPEE] üí° Run: POST /api/v1/shopee/session/auto-login")
            return CrawledProductDetail(link=product_url)
        
        print(f"[SHOPEE] üç™ Found {len(saved_cookies)} cookies")
        
        # ========================================
        # PH∆Ø∆†NG PH√ÅP 1: API TR·ª∞C TI·∫æP (NHANH!)
        # ========================================
        print("[SHOPEE] üöÄ Trying API method (faster)...")
        
        all_reviews = self._fetch_reviews_via_api(saved_cookies, shopid, itemid, review_limit)
        
        if all_reviews:
            print(f"[SHOPEE] ‚úÖ API method success! Got {len(all_reviews)} reviews")
            return CrawledProductDetail(
                link=product_url,
                category="",
                description="",
                detailed_rating={},
                total_rating=len(all_reviews),
                comments=all_reviews
            )
        
        # ========================================
        # PH∆Ø∆†NG PH√ÅP 2: SELENIUM (FALLBACK)
        # ========================================
        print("[SHOPEE] ‚ö†Ô∏è  API failed, trying Selenium fallback...")
        all_reviews = self._fetch_reviews_via_selenium(product_url, saved_cookies, shopid, itemid, review_limit)
        
        return CrawledProductDetail(
            link=product_url,
            category="",
            description="",
            detailed_rating={},
            total_rating=len(all_reviews),
            comments=all_reviews
        )
    
    def _fetch_reviews_via_api(
        self, 
        cookies: List[dict], 
        shopid: str, 
        itemid: str, 
        review_limit: int
    ) -> List[CrawledReview]:
        """
        Fetch reviews tr·ª±c ti·∫øp qua API (NHANH!)
        Ch·ªâ c·∫ßn cookies, kh√¥ng c·∫ßn Selenium
        """
        import requests
        
        all_reviews: List[CrawledReview] = []
        
        try:
            # Create session with cookies
            session = requests.Session()
            session.headers.update({
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "application/json",
                "Accept-Language": "vi-VN,vi;q=0.9",
                "Referer": f"https://shopee.vn/product-i.{shopid}.{itemid}",
            })
            
            # Apply cookies
            for c in cookies:
                if c.get('name') and c.get('value'):
                    session.cookies.set(
                        c['name'], 
                        c['value'], 
                        domain=c.get('domain', '.shopee.vn')
                    )
            
            # Fetch reviews
            reviews_api = "https://shopee.vn/api/v2/item/get_ratings"
            offset = 0
            limit = 20
            
            while len(all_reviews) < review_limit:
                params = {
                    "itemid": itemid,
                    "shopid": shopid,
                    "filter": 0,
                    "flag": 1,
                    "limit": limit,
                    "offset": offset,
                    "type": 0
                }
                
                print(f"[SHOPEE API] ÔøΩ Fetching reviews (offset={offset})...")
                resp = session.get(reviews_api, params=params, timeout=15)
                
                if resp.status_code != 200:
                    print(f"[SHOPEE API] ‚ùå Status {resp.status_code}")
                    return []  # Return empty to trigger fallback
                
                data = resp.json()
                
                # Check for API errors
                if data.get("error"):
                    print(f"[SHOPEE API] ‚ùå API error: {data.get('error')}")
                    return []
                
                ratings_data = data.get("data") or {}
                ratings = ratings_data.get("ratings") or []
                
                if not ratings:
                    print(f"[SHOPEE API] üì≠ No more reviews")
                    break
                
                for r in ratings:
                    if len(all_reviews) >= review_limit:
                        break
                    
                    images = r.get("images") or []
                    image_urls = [
                        f"https://down-vn.img.susercontent.com/{img}" for img in images
                    ]
                    
                    all_reviews.append(CrawledReview(
                        author=r.get("author_username") or "Anonymous",
                        rating=r.get("rating_star", 5),
                        content=r.get("comment") or "",
                        time=str(r.get("ctime", "")),
                        images=image_urls,
                        helpful_count=r.get("like_count", 0)
                    ))
                
                offset += len(ratings)
                print(f"[SHOPEE API] üìù Got {len(ratings)} reviews, total: {len(all_reviews)}")
                
                time.sleep(0.3)  # Small delay
            
            return all_reviews
            
        except Exception as e:
            print(f"[SHOPEE API] ‚ùå Error: {str(e)}")
            return []
    
    def _fetch_reviews_via_selenium(
        self,
        product_url: str,
        cookies: List[dict],
        shopid: str,
        itemid: str,
        review_limit: int
    ) -> List[CrawledReview]:
        """
        Fetch reviews via Selenium (FALLBACK)
        D√πng khi API b·ªã block
        """
        import requests
        
        driver = None
        all_reviews: List[CrawledReview] = []
        
        try:
            print("[SHOPEE SELENIUM] üöÄ Starting browser...")
            
            options = uc.ChromeOptions()
            # Non-headless ƒë·ªÉ bypass detection
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--lang=vi-VN")
            
            driver = uc.Chrome(options=options, version_main=None)
            
            # Load homepage and apply cookies
            driver.get(self.base_url)
            time.sleep(2)
            
            for cookie in cookies:
                try:
                    cookie_dict = {
                        'name': cookie.get('name'),
                        'value': cookie.get('value'),
                        'domain': cookie.get('domain', '.shopee.vn'),
                        'path': cookie.get('path', '/'),
                    }
                    if cookie_dict['name'] and cookie_dict['value']:
                        driver.add_cookie(cookie_dict)
                except:
                    pass
            
            # Load product page
            print("[SHOPEE SELENIUM] üìÑ Loading product page...")
            driver.get(product_url)
            time.sleep(5)
            
            # Scroll to reviews
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # Get fresh cookies from browser
            browser_cookies = driver.get_cookies()
            
            # Try API with browser cookies
            session = requests.Session()
            session.headers.update({
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Referer": product_url,
                "Accept": "application/json"
            })
            
            for c in browser_cookies:
                session.cookies.set(c['name'], c['value'])
            
            reviews_api = "https://shopee.vn/api/v2/item/get_ratings"
            offset = 0
            limit = 20
            
            while len(all_reviews) < review_limit:
                params = {
                    "itemid": itemid,
                    "shopid": shopid,
                    "filter": 0,
                    "flag": 1,
                    "limit": limit,
                    "offset": offset,
                    "type": 0
                }
                
                try:
                    resp = session.get(reviews_api, params=params, timeout=15)
                    if resp.status_code != 200:
                        break
                    
                    data = resp.json().get("data") or {}
                    ratings = data.get("ratings") or []
                    
                    if not ratings:
                        break
                    
                    for r in ratings:
                        if len(all_reviews) >= review_limit:
                            break
                        
                        images = r.get("images") or []
                        image_urls = [
                            f"https://down-vn.img.susercontent.com/{img}" for img in images
                        ]
                        
                        all_reviews.append(CrawledReview(
                            author=r.get("author_username") or "Anonymous",
                            rating=r.get("rating_star", 5),
                            content=r.get("comment") or "",
                            time=str(r.get("ctime", "")),
                            images=image_urls,
                            helpful_count=r.get("like_count", 0)
                        ))
                    
                    offset += len(ratings)
                    print(f"[SHOPEE SELENIUM] üìù Got {len(ratings)} reviews, total: {len(all_reviews)}")
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"[SHOPEE SELENIUM] ‚ö†Ô∏è  API error: {e}")
                    break
            
            # Update cookies
            self.cookie_manager.save_cookies(browser_cookies)
            print(f"[SHOPEE SELENIUM] ‚úÖ Total reviews: {len(all_reviews)}")
            
            return all_reviews
            
        except Exception as e:
            print(f"[SHOPEE SELENIUM] ‚ùå Error: {str(e)}")
            return []
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
